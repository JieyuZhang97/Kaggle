{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle - Titanic Survival Passengers Prediction \n",
    "\n",
    "This is my first Kaggle competition - <a herf=\"https://www.kaggle.com/c/titanic\"> predict the survival passengers from the Titanic</a>. In this project, I have learned \n",
    "- Data processing with **numpy and pandas** \n",
    "    - understand different types of data \n",
    "    - work with missing values \n",
    "    - feature engineering \n",
    "- Data visiualization with **seaborn**\n",
    "- Data prediction with **sklearn** \n",
    "    - decision tree \n",
    "    - kNN \n",
    "    - random forest\n",
    "    - logistic regression\n",
    "    - cross validation to determine the optimal approach \n",
    "\n",
    "The accuracy of my model is **85%** on dev set and **78%** on the final prediction set (<a herf=https://www.kaggle.com/jieyu7/competitions>check out my kaggle page</a>). Though I do find that simpler and easier feature engineering process gives a model with a higher accuracy. But the main goal of this project is to learn different techniques of data analysis. I will learn about the efficiency of different feature engineering methods in the next kaggle project. \n",
    "\n",
    "2020.05.31\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data \n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "**************************************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_df.info())\n",
    "print('*'*50)\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unrelated columns \n",
    "\n",
    "The first step is to look at the data and remove any unrelated columns. Here only the `PassengerId` is unrelated and has no influence on the prediction.\n",
    "\n",
    "`PassengerId` is removed from both the training and the test sets but saved as a separate column for creating the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PassengerId = test_df['PassengerId']\n",
    "\n",
    "train_df.drop(['PassengerId'], axis = 1, inplace = True)\n",
    "test_df.drop(['PassengerId'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group features \n",
    "\n",
    "The second step is to find out the data type of each feature, and whether each feature has unique values or missing values. The following is a brief summary. This summary is a guidance of the following data processing steps. This guidance could be changed as my understanding of the data set gets deeper. \n",
    "\n",
    "**bold** : missing values\n",
    "\n",
    "*italic* : unique values\n",
    "\n",
    "- numertical features (shown in numbers)\n",
    "    1. Pclass\n",
    "    2. **Age**\n",
    "    3. SibSp\n",
    "    4. Parch\n",
    "    5. **Fare** \n",
    "- categorical features (shown in letters) : convert to numbers and get dummies \n",
    "    1. Sex\n",
    "    2. **Embarked**\n",
    "- alphanumerical/text features (shown in both letters and numbers) : engineering features \n",
    "    1. **Cabin**\n",
    "    2. *Ticket*\n",
    "    3. *Name*\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "# Process the data \n",
    "\n",
    "To process the data, I start with the simpler tasks and continue with harder tasks. The steps are \n",
    "1. convert categorical values to numbers\n",
    "2. work with missing values \n",
    "3. work with feature engineering \n",
    "4. convert categorical values to dummies \n",
    "\n",
    "I choose not to convert the categorical values to dummies at first because this would create too many columns that makes the visualization complex. Before working on the data. I combine the `train_df` and `test_df` together as `all_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass                                               Name  \\\n",
       "0         0       3                            Braund, Mr. Owen Harris   \n",
       "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2         1       3                             Heikkinen, Miss. Laina   \n",
       "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4         0       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1  female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4    male  35.0      0      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train_df,test_df], ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "## Convert categorical features to numbers \n",
    "There are two categorical features: `Sex` and `Embarked`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n",
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "print(all_data['Sex'].unique())\n",
    "print(all_data['Embarked'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `Embarked` has missing values, we need to work with the `nan` first and them factorize it. So in this step. only `Sex` is converted to numberical values where `male` is `0` and `female` is `1`. Both `train_df` and `test_df` are included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Sex'] = pd.factorize(all_data['Sex'])[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with missing values \n",
    "\n",
    "I use a simple function to print the percentage of missing values in each feature. This function is from another kaggle post that I cannot find now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def missing_percentage(df):\n",
    "    \"\"\"\n",
    "        This function takes a DataFrame(df) as input and returns two columns, \n",
    "        total missing values and total missing values percentage\n",
    "    \"\"\"\n",
    "    total = df.isnull().sum().sort_values(ascending = False)\n",
    "    percent = round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentage(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentage(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentage(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both in `test_df` and in `train_df`, features `Cabin` and `Age` have missing values. 2 missing value in `Embarked` and 1 missing value in `Fare`. I will first look into `Fare` and `Embarked`. For the other two features, because there are so many missing values, it is highly likely there will be feature engineering involved. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embarked \n",
    "The following is another function from the same kaggle post mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_value_counts(df, feature):\n",
    "    \"\"\"This function takes in a dataframe and a column and finds the percentage of the value_counts\"\"\"\n",
    "    percent = pd.DataFrame(round(df.loc[:,feature].value_counts(dropna=False, normalize=True)*100,2))\n",
    "    ## creating a df with th\n",
    "    total = pd.DataFrame(df.loc[:,feature].value_counts(dropna=False))\n",
    "    ## concating percent and total dataframe\n",
    "\n",
    "    total.columns = [\"Total\"]\n",
    "    percent.columns = ['Percent']\n",
    "    return pd.concat([total, percent], axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_value_counts(train_df, 'Embarked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the majority of `Embarked` is `S`, it is reasonable to fill the missing values as `S`. But I will plot the relation between `Embarked` and `Fare` and `Pclass` to see if `S` is reasonable. Because I suspect the `Embarked` is related to these two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[all_data.Embarked.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the missing vales, they are both female, in `Pclass`1 and `Fare` of 80 ; I use seaborn plots to see if `Pclass` and `Fare` are related to `Embarked` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=all_data);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with `Pclass` = 1 and `Fare` = 80, it is more likely to embark at C. Therefore, I will fill the missing values with C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Embarked'].fillna(\"C\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filling the missing values, this feature is again factorized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Embarked'] = pd.factorize(all_data['Embarked'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fare\n",
    "\n",
    "there is only one missing value of fare in test_df\n",
    "see how it is like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[all_data.Fare.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pclass` = 3 and `Embarked` at 0, from the above figure we can roughly estimate the fare as the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a simpler version \n",
    "\n",
    "# missing_value_fare = all_data[(all_data['Pclass'] == 3) & (all_data['Embarked'] == 0)].Fare.mean()\n",
    "# ## replace the test.fare null values with test.fare mean\n",
    "# all_data.Fare.fillna(missing_value_fare, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering feature \n",
    "\n",
    "Features need to be engineered\n",
    "\n",
    "1. Cabin: a letter + some number; the letter could be used \n",
    "2. Name: mainly use the titles of the names \n",
    "3. Ticket: see if there is anything we can use \n",
    "4. Age: missing value with ML; see if needed to group them \n",
    "\n",
    "Features could be engineered\n",
    "1. SibSp\n",
    "2. parch \n",
    "<div>\n",
    "some ideas\n",
    "\n",
    "    - put into different groups rather than continuous values \n",
    "    - put together as family size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cabin \n",
    "\n",
    "The naming convention of the `Cabin` is a letter followed by some numbers. People with multiple cabins still have the same letter. So we can just use the first letter as the feature. The `Cabin` is directly associated with class, according to an answer from <a herf=\"https://www.quora.com/What-were-the-ticket-prices-to-board-the-Titanic\">Quora</a>. Therefore, the missing values in `Cabin` can be determined by its fare and class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cabin with values \n",
    "all_cabin_nonan = all_data[all_data['Cabin'].notnull()]\n",
    "\n",
    "all_cabin_nonan['Cabin'] = [i[0] for i in all_cabin_nonan['Cabin']]\n",
    "\n",
    "all_cabin_nonan = all_cabin_nonan.sort_values(by='Cabin')\n",
    "\n",
    "sns.catplot(x=\"Cabin\", y=\"Fare\", hue='Pclass', kind=\"swarm\", data=all_cabin_nonan);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, in each kind, the `Fare` has a large range. To better predict the missing values, here I decide to use bootstrap to generate values. For each `Fare` range and `Pclass` group, the missing `Cabin` value is randomly selected from the known distribution. For example, For `Pclass` = 1 and `Fare` between 200-300, the missing `Cabin` value is chosen from the distribution of values that satisfy this condition. Therefore, I need to decide the group criterion. \n",
    "\n",
    "pseudo code:\n",
    "\n",
    "`if Pclass == 3:\n",
    "    if Fare > 20:  # this threshold is determined in the following code \n",
    "        Cabin = F \n",
    "    else: Cabin = random.select from dist of Cabin == E, F, G & Fare < 20 and  Pclass == 3\n",
    " if Pclass == 2:\n",
    "     \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "`\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find the threshold from F column\n",
    "all_cabin_nonan[[\"Fare\",\"Pclass\"]][(all_cabin_nonan[\"Cabin\"]=='F') & (all_cabin_nonan[\"Pclass\"]==3)].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cabin_nonan[\"Cabin\"] = pd.factorize(all_cabin_nonan[\"Cabin\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cabin_nonan[\"Cabin\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cabin_nan = all_data[all_data['Cabin'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin=np.zeros((all_data['Cabin'].isnull().sum(),1))\n",
    "all_cabin_pclass3 = all_cabin_nonan[all_cabin_nonan[\"Pclass\"] == 3]\n",
    "all_cabin_pclass2 = all_cabin_nonan[all_cabin_nonan[\"Pclass\"] == 2]\n",
    "all_cabin_pclass1 = all_cabin_nonan[all_cabin_nonan[\"Pclass\"] == 1]\n",
    "\n",
    "for i in range(len(cabin)):\n",
    "    data = all_cabin_nan.iloc[i]\n",
    "    if data['Pclass'] == 3:\n",
    "        if data['Fare'] > 20:\n",
    "            cabin[i] = 5\n",
    "        else:\n",
    "            group = all_cabin_pclass3[all_cabin_pclass3['Fare'] <= 20]\n",
    "            dist = group['Cabin'][(group[\"Cabin\"] >= 3) & (group[\"Cabin\"] <= 5)]\n",
    "            cabin[i] = np.random.choice(dist)\n",
    "    elif data['Pclass'] == 2:\n",
    "        if data['Fare'] > 20:\n",
    "            cabin[i] = 5\n",
    "        else:\n",
    "            group = all_cabin_pclass2[all_cabin_pclass2['Fare'] <= 20]\n",
    "            dist = group['Cabin'][(group[\"Cabin\"] >= 2) & (group[\"Cabin\"] <= 5)]\n",
    "            cabin[i] = np.random.choice(dist)\n",
    "    else:\n",
    "        if data['Fare'] > 500:\n",
    "            cabin[i] = 1\n",
    "        elif data['Fare'] > 200:\n",
    "            group = all_cabin_pclass1[all_cabin_pclass1['Fare'] > 200]\n",
    "            dist = group['Cabin'][(group[\"Cabin\"] >= 1) & (group[\"Cabin\"] <= 2)]\n",
    "            cabin[i] = np.random.choice(dist)\n",
    "        elif data['Fare'] < 100:\n",
    "            group = all_cabin_pclass1[all_cabin_pclass1['Fare'] < 100]\n",
    "            dist = group['Cabin'][(group[\"Cabin\"] >= 0) & (group[\"Cabin\"] <= 4)]\n",
    "            # cabin T is ignored because there is only one of it -> statistically very small\n",
    "            cabin[i] = np.random.choice(dist)\n",
    "        else:\n",
    "            group = all_cabin_pclass1[(all_cabin_pclass1['Fare'] <= 200) & (all_cabin_pclass1['Fare'] >= 100)]\n",
    "            dist = group['Cabin'][(group[\"Cabin\"] >= 1) & (group[\"Cabin\"] <= 4)]\n",
    "            cabin[i] = np.random.choice(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cabin_nan['Cabin'] = cabin\n",
    "\n",
    "all_data = pd.concat([all_cabin_nonan,all_cabin_nan], ignore_index=False) \n",
    "\n",
    "all_data['Cabin'] = all_data['Cabin'].astype(int)\n",
    "# train_df_new = all_data_new[all_data_new['Survived'].notnull()]\n",
    "# test_df_new = all_data_new[all_data_new['Survived'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name \n",
    "\n",
    "For the `Name`, I will extract the title as a new feature and get rid of the `Name` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Title']=all_data.Name.apply(lambda x: x.split('.')[0].split(',')[1].strip())\n",
    "all_data['Title'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I initially found a method of creating the `Title` with 4 different categories: Mr, Miss, Mrs and Rare, where Rare includes all other different titles such as Dr. Then I realized that simply putting all other titles as `Rare` is ineffective and could impact the accuracy when predicting `Age` as `Age` appears to be associated with titles. Hence I found another way of creating the `Title` where it has more categories from this <a herf=\"https://medium.com/i-like-big-data-and-i-cannot-lie/how-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9\">post<a/>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is the first one. \n",
    "# all_data_new['Title'] = all_data_new['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    "#  \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare') ## simply put all the rare titles in one group is not wise. because the title is important in the Age prediction later! \n",
    "\n",
    "# all_data_new['Title'] = all_data_new['Title'].replace('Mlle', 'Miss') # Mlle to Miss\n",
    "# all_data_new['Title'] = all_data_new['Title'].replace('Ms', 'Miss') # Ms to Miss\n",
    "# all_data_new['Title'] = all_data_new['Title'].replace('Mme', 'Mrs') # Mme to Mrs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the second one. \n",
    "newtitles={\n",
    "    \"Capt\":       \"Officer\",\n",
    "    \"Col\":        \"Officer\",\n",
    "    \"Major\":      \"Officer\",\n",
    "    \"Jonkheer\":   \"Royalty\",\n",
    "    \"Don\":        \"Royalty\",\n",
    "    \"Sir\" :       \"Royalty\",\n",
    "    \"Dr\":         \"Officer\",\n",
    "    \"Rev\":        \"Officer\",\n",
    "    \"the Countess\":\"Royalty\",\n",
    "    \"Dona\":       \"Royalty\",\n",
    "    \"Mme\":        \"Mrs\",\n",
    "    \"Mlle\":       \"Miss\",\n",
    "    \"Ms\":         \"Mrs\",\n",
    "    \"Mr\" :        \"Mr\",\n",
    "    \"Mrs\" :       \"Mrs\",\n",
    "    \"Miss\" :      \"Miss\",\n",
    "    \"Master\" :    \"Master\",\n",
    "    \"Lady\" :      \"Royalty\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Title']=all_data.Title.map(newtitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure there is no missing values \n",
    "all_data['Title'][all_data['Title'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the Name feature and factorize the Title feature \n",
    "all_data.drop(['Name'], axis=1, inplace=True)\n",
    "\n",
    "all_data[\"Title\"] = pd.factorize(all_data[\"Title\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ticket\n",
    "\n",
    "I have not yet found an effective approach of dealing with `Ticket`, hence here I choose to drop it. I looked into some discussion posts on kaggle but I think they requires more energy than necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Ticket'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket = all_data['Ticket']\n",
    "\n",
    "all_data.drop(['Ticket'], axis=1, inplace=True)\n",
    "\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age \n",
    "\n",
    "Because we need to use `Age` as a feature to predict survival rate, `Survival` is not used here to predict age. (not sure if this is correct. But if survival rate is used to predict age and then age is again used to predict survival, wouldn't it be a circulation?)\n",
    "\n",
    "My first thought is to find out if there is any relation between other features and `Age` by plotting using seaborn. If the distribution of age significantly differs from the feature, this feature is included in the prediction of `Age`. <div>\n",
    "Features included here are `Sex, Pclass, Cabin, Embarked, Title,` and `Fare`. `SibSp` and `Parch` are not included because there is no direct rational relation among these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Sex','Pclass','Cabin','Embarked','Title','Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "fig, axes = plt.subplots(ncols=2,nrows=3, figsize=(20,12))\n",
    "axes = axes.flatten()\n",
    "for feature in features:\n",
    "    if feature == 'Fare':\n",
    "        sns.scatterplot(x='Age', y=feature, data=all_data,legend=False, ax = axes[i])\n",
    "    else:\n",
    "        y_axis = all_data[feature].unique()\n",
    "        for y in y_axis:\n",
    "            sns.kdeplot(all_data.loc[(all_data[feature] == y),'Age'] , shade=True,label= feature +\" \"+ str(y), ax = axes[i]);\n",
    "    #     axes[i].title('Age Distribution '+ feature, fontsize = 25, pad = 40)\n",
    "    #     axes[i].xlabel(\"Age\", fontsize = 15, labelpad = 20)\n",
    "    #     axes[i].ylabel('Frequency', fontsize = 15, labelpad= 20);\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see that features including `Pclass, Cabin`, and `Title`. \n",
    "\n",
    "However, after some tests, the random forest method is not working that well in the prediction. Then I decide to assign the mean of the `Title` feature to the corresponding missing value. This idea is inspired by this <a herf=\"https://www.kaggle.com/goldens/titanic-on-the-top-with-a-simple-model\"> discussion<a/>.\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"Title\", y=\"Age\", hue='Sex', kind=\"swarm\", data=all_data[all_data['Survived'].notnull()]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_predict_mean = all_data.groupby(['Title','Sex']).Age.mean()\n",
    "\n",
    "age_predicts = all_data[all_data.Age.isnull()]\n",
    "\n",
    "for ind in age_predict_mean.index:\n",
    "    age_predicts['Age'][(age_predicts['Title'] == ind[0])&(age_predicts['Sex'] == ind[1])] = age_predict_mean.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_known = all_data[all_data.Age.notnull()]\n",
    "\n",
    "all_data = pd.concat([age_known,age_predicts], ignore_index=False) \n",
    "\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the features have no missing values except for `Survived`. The rows from `test_df` have no `Survived`. Next I will combine `SibSp` and `Parch` into a new feature as the family size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SibSp & parch\n",
    "\n",
    "I will first plot the relations to see if it is reasonable to combine these two features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature called the family size to see if this is relavant \n",
    "all_data['FamSiz'] = all_data['Parch'] + all_data['SibSp'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"SibSp\", y=\"Parch\", hue='Survived', kind=\"swarm\", data=all_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"Survived\", y=\"FamSiz\", kind=\"swarm\", data=all_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot on the top does not show obvious groups of `Parch` or `SibSp` with survival. But the bottom plot shows that if you are a family with more than 7 people, judging from the plot, the chance of living is zero. Hence I can create a feature called `FamSiz_7` as familysize=7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['FamSiz_7'] = (all_data['FamSiz'] > 6).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(['FamSiz'], axis=1, inplace=True)\n",
    "\n",
    "all_data.drop(['SibSp','Parch'],axis=1,inplace=True)\n",
    "\n",
    "all_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the data (get dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for categorical features\n",
    "pclass_dummies = pd.get_dummies(all_data.Pclass, prefix=\"Pclass\")\n",
    "title_dummies = pd.get_dummies(all_data.Title, prefix=\"Title\")\n",
    "cabin_dummies = pd.get_dummies(all_data.Cabin, prefix=\"Cabin\")\n",
    "embarked_dummies = pd.get_dummies(all_data.Embarked, prefix=\"Embarked\")\n",
    "\n",
    "# concatenate dummy columns with main dataset\n",
    "all_data_dummies = pd.concat([all_data, pclass_dummies, title_dummies, cabin_dummies, embarked_dummies], axis=1)\n",
    "\n",
    "# drop categorical fields\n",
    "all_data_dummies.drop(['Pclass', 'Title', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
    "\n",
    "all_data_dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_dummies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "For the model training, I adopted four different methods\n",
    "    - decision tree\n",
    "    - kNN\n",
    "    - random forest\n",
    "    - logstic regression \n",
    "\n",
    "The following codes are based applications of these four methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the train data and test data are correct in length \n",
    "print(all_data.Survived.notnull().sum())\n",
    "print(all_data.Survived.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_data[all_data.Survived.notnull()]\n",
    "test_data = all_data[all_data.Survived.isnull()]\n",
    "y = train_data['Survived']\n",
    "train_data.drop(['Survived'],axis=1,inplace=True)\n",
    "test_data.drop(['Survived'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "scaler = StandardScaler()\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create train data and dev data \n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(train_data.values, y, test_size=0.3,\n",
    "random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5, random_state=17)\n",
    "\n",
    "\n",
    "tree_params = {'max_depth': range(2,7),\n",
    "               'max_features': range(2,15)}\n",
    "\n",
    "tree_grid = GridSearchCV(tree, tree_params,\n",
    "                         cv=5, n_jobs=-1, verbose=True) \n",
    "\n",
    "tree_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for kNN, we need to scale features\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "scaler = StandardScaler()  \n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_holdout_scaled = scaler.transform(X_holdout)\n",
    "knn_pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\n",
    "knn_params = {'knn__n_neighbors': range(1, 10)}\n",
    "knn_grid = GridSearchCV(knn_pipe, knn_params, cv=5, n_jobs=-1, verbose=True)\n",
    "knn_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=17)\n",
    "forest_params = {'max_depth': range(2,12),\n",
    "                 'max_features': range(2,12)}\n",
    "\n",
    "forest_grid = GridSearchCV(forest, forest_params,\n",
    "                           cv=5, n_jobs=-1, verbose=True)\n",
    "\n",
    "forest_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg=LogisticRegression()\n",
    "logreg_grid=GridSearchCV(logreg,grid,cv=10)\n",
    "logreg_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the best method\n",
    "\n",
    "The most appropriate method is the one with the highest dev score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_score = [accuracy_score(y_holdout, tree_grid.predict(X_holdout)),\n",
    "             accuracy_score(y_holdout, knn_grid.predict(X_holdout)),\n",
    "             accuracy_score(y_holdout, forest_grid.predict(X_holdout)),\n",
    "             accuracy_score(y_holdout, logreg_grid.predict(X_holdout)),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision Tree:')\n",
    "# print('best parameters: ',tree_grid.best_params_) #{'max_depth': 6, 'max_features': 17}\n",
    "print('best score: ',tree_grid.best_score_ )\n",
    "print('Dev score: ',accuracy_score(y_holdout, tree_grid.predict(X_holdout)))\n",
    "print('kNN:')\n",
    "# print('best parameters: ',knn_grid.best_params_) #{'max_depth': 6, 'max_features': 17}\n",
    "print('best score: ',knn_grid.best_score_ )\n",
    "print('Dev score: ',accuracy_score(y_holdout, knn_grid.predict(X_holdout)))\n",
    "print('Random Forest:')\n",
    "# print('best parameters: ',forest_grid.best_params_) #{'max_depth': 6, 'max_features': 17}\n",
    "print('best score: ',forest_grid.best_score_ )\n",
    "print('Dev score: ',accuracy_score(y_holdout, forest_grid.predict(X_holdout)))\n",
    "print('Logistic Regression:')\n",
    "# print('best parameters: ',forest_grid.best_params_) #{'max_depth': 6, 'max_features': 17}\n",
    "print('best score: ',logreg_grid.best_score_ )\n",
    "print('Dev score: ',accuracy_score(y_holdout, logreg_grid.predict(X_holdout)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that decision tree gives the highest score. So I will use this model for prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_survive = tree_grid.predict(test_data)\n",
    "test_data['Survive'] = test_survive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'PassengerId': PassengerId, 'Survived': test_data['Survive'].astype(int)})\n",
    "output.to_csv('my_submission_v9.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prediction gives 78% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.273px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.545454,
   "position": {
    "height": "40px",
    "left": "1028.34px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
